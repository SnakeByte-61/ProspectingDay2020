{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker/DeepAR demo on a restaurant dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepAR stands for Deep Autoregression. Autoregression is used to model time-series with autocorrelation, i.e. the succesive values in time series are not independent of each other. In DeepAR, we exploit the power of recurrent neural network to model the complexity of time-sequences. The model learns to forecast parameters of a chosen distribution, which is most likely to generate the target values. One can also add external regressors to the model. The built-in algorithm identifies different levels of seasonality present in the time series data on its own.         \n",
    "\n",
    "For more information see the DeepAR [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html) or [paper](https://arxiv.org/abs/1704.04110)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index \n",
    "Here, we will consider a real use case and show how to use DeepAR on SageMaker for forecasting sales of 4 products accross 25 stores for a recursive period of 2 weeks.\n",
    "\n",
    "In particular, we will see how to:\n",
    "\n",
    "* Prepare the dataset.\n",
    "    * [Import ProductStore dataset](#Import-ProductStore-dataset)\n",
    "    * [Adding extraneous variables](#Adding-extraneous-variables)\n",
    "    * [Data Preprocessing](#Data-Preprocessing)\n",
    "    * [Data Visualization](#Data-Visualization)\n",
    "\n",
    "* Use the SageMaker Python SDK to train a DeepAR model and deploy it.\n",
    "    * [Preparing for Sagemaker Training and Inference](#Preparing-for-Sagemaker-Training-and-Inference)\n",
    "    * [Train and Test splits](#Train-and-Test-splits)\n",
    "    * [DeepAR Container](#DeepAR-Container)\n",
    "    * [Sagemkaer Training](#Sagemaker-Training)\n",
    "    * [Create endpoint and predictor](#Create-endpoint-and-predictor)\n",
    "    \n",
    "* Make requests to the deployed model to obtain forecasts interactively\n",
    "    * [Make predictions and plot results](#Make-predictions-and-plot-results)\n",
    "\n",
    "* Clean up\n",
    "    * [Delete endpoints](#Delete-endpoints)\n",
    "\n",
    "Note: We will be using us-east-1 region for all our exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing required package\n",
    "!pip install holidays  # to install the holidays package\n",
    "\n",
    "# Importing required modules\n",
    "import warnings\n",
    "import deeparpredictor\n",
    "import holidays\n",
    "from ipywidgets import IntSlider, FloatSlider, Checkbox\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import boto3\n",
    "import time\n",
    "from random import randint\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "from dateutil.parser import parse\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import ProductStore dataset \n",
    "The data is made available to you in the folder.\n",
    "\n",
    "The data is originally recorded at day level intervals for combination of stores and products. Since the data is already at day level interval, we can directly load and process the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the raw dataset\n",
    "data = pd.read_csv('data_raw.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'], dayfirst=True)\n",
    "data = data.sort_values(by=['Store_ID', 'Product_ID', 'Date'])\n",
    "data = data.reset_index()\n",
    "data = data.drop(['index'], axis=1)\n",
    "data['Turnover'] = data['SalesCount'] * data['Price']\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding extraneous variables\n",
    "As we will observe later in this notebook, there are multiple seasonality factors in the data like weekly seasonality, yearly seasonality etc. But there are certain special days that appear on irregular intervals whose impact is visible on sales. We will add them to the model as dynamic features. The following snippet adds columns for these dynamic features into our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating binary variable for BlackFriday\n",
    "us_holidays = holidays.UnitedStates()\n",
    "\n",
    "data['Holiday'] = [us_holidays.get(item) for item in data['Date']]\n",
    "data['Thanksgiving'] = data['Holiday'] == 'Thanksgiving'\n",
    "data['BlackFriday'] = [\n",
    "    1 if item else 0 for item in data['Thanksgiving'].shift(1).fillna(False)]\n",
    "\n",
    "data = data.drop(['Holiday', 'Thanksgiving'], axis=1)\n",
    "ext_features = ['BlackFriday', 'AnnualSalesWeek']\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "We convert the data to a collection of Pandas time series. It makes common time series operations such as indexing by time periods or resampling much easier. Once the data is processed, each store product combination will have a seperate timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing all the unique stores and products present in the data\n",
    "prod_dict = dict(enumerate(data.Product_ID.unique()))\n",
    "store_dict = dict(enumerate(data.Store_ID.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = []  # will store the unique combination of ProductID and StoreID\n",
    "timeseries = []  # will store the series\n",
    "dynamic_feat = []  # will store the continuous independent variables of the data\n",
    "\n",
    "ts_dict = {}  # Stores index of timeseries in the list corresponding to indices of store and product\n",
    "\n",
    "for i in store_dict:\n",
    "    for j in prod_dict:\n",
    "        df = data[(data['Store_ID'] == store_dict[i]) &\n",
    "                  (data['Product_ID'] == prod_dict[j])]\n",
    "        df = df.set_index('Date')\n",
    "\n",
    "        # begin triming from 1st nonzero value\n",
    "        series = np.trim_zeros(df.loc[:, 'SalesCount'], trim='f')\n",
    "        timeseries.append(series)\n",
    "        dynamic_feat.append(df.loc[series.index, ext_features])\n",
    "        cat.append([i, j])\n",
    "        ts_dict[i, j] = len(timeseries) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization \n",
    "Plotting the resulting time series. We observe that there is weekly seasonality in the timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 1, figsize=(20, 20), sharex=True)\n",
    "axx = axs.ravel()\n",
    "for i in range(0, 5):\n",
    "    timeseries[i].loc[\"2014-01-01\":\"2014-02-15\"].plot(ax=axx[i])\n",
    "    axx[i].set_xlabel(\"Date\", fontsize=18)\n",
    "    axx[i].set_ylabel(\"Sales Count\", fontsize=18)\n",
    "    axx[i].grid(which='minor', axis='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also observe that there is yearly seasonality in the data. There are relative spikes in sales on Blackfriday and during AnnualSalesWeek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, figsize=(20, 20), sharex=True)\n",
    "axx = axs.ravel()\n",
    "for i in range(0, 5):\n",
    "    timeseries[i].loc[\"2014-01-01\":\"2016-09-30\"].plot(ax=axx[i])\n",
    "    axx[i].set_xlabel(\"Date\", fontsize=18)\n",
    "    axx[i].set_ylabel(\"Sales Count\", fontsize=18)\n",
    "    axx[i].grid(which='minor', axis='x')\n",
    "\n",
    "    ax2 = axx[i].twinx()\n",
    "    dynamic_feat[i].loc[\"2014-01-01\":\"2016-09-30\",\n",
    "    'BlackFriday'].plot(ax=ax2, color='r', legend='BlackFriday')\n",
    "    ax2.set_ylim(-0.2, 7)\n",
    "\n",
    "    ax3 = axx[i].twinx()\n",
    "    dynamic_feat[i].loc[\"2014-01-01\":\"2016-09-30\",\n",
    "    'AnnualSalesWeek'].plot(ax=ax2,\n",
    "                            color='y',\n",
    "                            legend='AnnualSalesWeek')\n",
    "    ax3.set_ylim(-0.2, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing for Sagemaker Training and Inference\n",
    "Before starting, we create a S3 bucket and set the IAM role to give access for training and hosting the data.\n",
    "After bucket creation, it can be seen on the console [here.](https://s3.console.aws.amazon.com/s3/home?region=us-east-1#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new bucket\n",
    "client = boto3.client('s3')\n",
    "st = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "bucketName = 'deepar-sagemaker-demo-' + str(randint(1, 999999)) + '-' + st\n",
    "\n",
    "res = client.create_bucket(Bucket=bucketName)\n",
    "s3_bucket = bucketName\n",
    "# prefix used for all data stored within the bucket\n",
    "s3_prefix = 'deepar-demand-forecasting'\n",
    "\n",
    "print(\"Bucket Created with name:\", bucketName)\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "region = sagemaker_session.boto_region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "\n",
    "def copy_to_s3(local_file, s3_path, override=False):\n",
    "    assert s3_path.startswith('s3://')\n",
    "    split = s3_path.split('/')\n",
    "    bucket = split[2]\n",
    "    path = '/'.join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print(\n",
    "                'File s3://{}/{} already exists.\\nSet override to upload anyway.\\n'.format(\n",
    "                    s3_bucket, s3_path))\n",
    "            return\n",
    "        else:\n",
    "            print('Overwriting existing file')\n",
    "    with open(local_file, 'rb') as data:\n",
    "        print('Uploading file to {}'.format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)\n",
    "\n",
    "\n",
    "s3_data_path = \"s3://{}/{}-data/data\".format(\n",
    "    s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}-data/output\".format(\n",
    "    s3_bucket, s3_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify some high level features like the frequency, prediction length(number of days) and context length for timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use Daily frequency for the time series\n",
    "freq = 'D'\n",
    "\n",
    "# we predict for the next 14 days\n",
    "prediction_length = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test splits\n",
    "\n",
    "Often times one is interested in evaluating the model or tuning its hyperparameters by looking at error metrics on a hold-out test set. Here we split the available data into train and test sets for evaluating the trained model. For standard machine learning tasks such as classification and regression, one typically obtains this split by randomly separating examples into train and test sets. However, in forecasting it is important to do this train/test split based on time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dataset = min(data['Date'])  # setting start data for training\n",
    "end_training = datetime.datetime.strptime(\n",
    "    '2016-8-10',\n",
    "    '%Y-%m-%d') - datetime.timedelta(91)  # setting end data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DeepAR JSON input format represents each time series as a JSON object. In the simplest case each time series just consists of a start time stamp (``start``) and a list of values (``target``). For more complex cases, DeepAR also supports the fields ``dynamic_feat`` for time-series features and ``cat`` for categorical features. We shall insert Blackfriday and AnnualSalesWeek as dynamic features, store and product indices as categorical features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model creates embeddings for categorical features. We shall exclude some of the store-product combinations from the training data. The idea is to evaluate if the model can learn to forecast for these combinations by interaction of embeddings learnt through other occurances of the store or the product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = [[22, 3], [4, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]\n",
    "\n",
    "\n",
    "train_data = [{\"start\": str(start_dataset),\n",
    "               \"target\": encode_target(timeseries[ts_dict[i, j]][start_dataset:end_training]),\n",
    "               \"cat\": [i, j],\n",
    "               \"dynamic_feat\": [dynamic_feat[ts_dict[i, j]].loc[start_dataset:end_training, feat].tolist() for feat in\n",
    " ext_features]} for i, j in cat if cat not in exclude]\n",
    "\n",
    "print(len(train_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As test data, we have considered time series extending beyond the training range for a period of 8 weeks. These will be used for computing test scores, by using the trained model to forecast the succeeding 8 weeks, and comparing predictions with actual values.\n",
    "To evaluate our model performance on more than one week, we generate test data that extends for 8 number of weeks beyond the training range. This way we perform *rolling evaluation* of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_windows = 4\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": encode_target(\n",
    "            timeseries[ts_dict[i, j]][start_dataset:end_training + datetime.timedelta(k * prediction_length)]),\n",
    "        \"cat\": [i, j],\n",
    "        \"dynamic_feat\": [\n",
    "            dynamic_feat[ts_dict[i, j]].loc[start_dataset:end_training + datetime.timedelta(k * prediction_length),\n",
    "            feat].tolist() for feat in ext_features]\n",
    "    }\n",
    "    for k in range(1, num_test_windows + 1)\n",
    "    for i, j in cat\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now write the dictionary to the `jsonlines` file format that DeepAR understands (it also supports gzipped jsonlines and parquet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, 'wb') as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode('utf-8'))\n",
    "\n",
    "\n",
    "# storing files locally\n",
    "write_dicts_to_file(\"train_data.json\", train_data)\n",
    "write_dicts_to_file(\"test_data.json\", test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data files locally, let us copy them to S3 where Sagemaker can access them. Using the boto3 library we upload the data to the bucket.\n",
    "You can see the files in your bucket on the console [here.](https://s3.console.aws.amazon.com/s3/home?region=us-east-1#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uplaoding file to S3\n",
    "print('Uploading to S3 this may take a few minutes depending on your connection.')\n",
    "copy_to_s3(\"train_data.json\", s3_data_path + \"/train/train_data.json\", override=True)\n",
    "copy_to_s3(\"test_data.json\", s3_data_path + \"/test/test_data.json\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the S3-location of training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_dict = {'train_data_location': \"{}/train/\".format(s3_data_path),\n",
    "                 'test_data_location': \"{}/test/\".format(s3_data_path), 's3_output_path': \"s3://{}/{}-data/output\"\n",
    "                 .format(s3_bucket, s3_prefix)}\n",
    "\n",
    "with open(\"location.json\", 'wb') as fp:\n",
    "    fp.write(json.dumps(location_dict).encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepAR Container\n",
    "We pull the container image to be used for the region that we are running in. This contains the DeepAR model developed by Amazon.\n",
    "\n",
    "The advantage of using a built-in algorithm is that you don't have to worry about the environment setup, the internal working of the algorithm, etc. A user with limited understanding of machine learning can also implement these techniques on their own data. The inference can be obtained through very simple api call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sagemaker Training\n",
    "We are ready to launch the training job. SageMaker will start an EC2 instance, download the data from S3, start training the model and save the trained model.\n",
    "\n",
    "If you provide the `test` data channel as we do in this example, DeepAR will also calculate accuracy metrics for the trained model on this test. This is done by predicting the last `prediction_length` points of each time-series in the test set and comparing this to the actual value of the time-series. \n",
    "\n",
    "**Note:** The next cell may take a few minutes to complete, depending on data size, model complexity, training options.\n",
    "You can also see the progress of the training job in the console [here.](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"context_length\": '14',\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"epochs\": \"500\",\n",
    "    \"likelihood\": \"negative-binomial\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "}\n",
    "\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='deepar-timeseries',\n",
    "    output_path=s3_output_path,\n",
    "    hyperparameters=hyperparameters,\n",
    "    tags=[{\"Key\": \"project\", \"Value\": \"Sagemaker-Demo\"}]\n",
    ")\n",
    "\n",
    "\n",
    "estimator.fit(\n",
    "    inputs={\n",
    "        \"train\": location_dict['train_data_location'],\n",
    "        \"test\": location_dict['test_data_location']\n",
    "    },\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint and predictor\n",
    "\n",
    "Now that we have a trained model, we can use it to perform predictions by deploying it to an endpoint.\n",
    "\n",
    "**Note: Remember to delete the endpoint after running this experiment. A cell at the very bottom of this notebook will do that: make sure you run it at the end.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query the endpoint and perform predictions, we can define the following utility class, which allows us to make requests using `pandas.Series` objects rather than raw JSON strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can deploy the model and create and endpoint that can be queried using our custom DeepARPredictor utility class.\n",
    "You can also see the progress of the endpoint creation in the console [here.](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# creating the endpoint\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,  \n",
    "    instance_type='ml.m4.xlarge',  \n",
    "    predictor_cls=deeparpredictor.DeepARPredictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions and plot results\n",
    "\n",
    "Now we can use the `predictor` object to generate predictions.\n",
    "\n",
    "We can interact with sliders to look at the forecast of any store product combination. `forecast_day` will specify the number of days after end_training after which to begin forecasting. With `confidence`, we can set the range of prediction interval for the negative-binomial distribution of the target variable. For each request, the predictions are obtained by calling our served model on the fly.\n",
    " \n",
    "Click on `Run Interact` to generate the predictions from the endpoint and see the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization shows line chart of actual vs predicted time-series. The blue line is 50-percentile value of the predicted distribution, the shaded region is prediction interval of the predicted distribution of target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = {'description_width': 'initial'}\n",
    "\n",
    "store_id = 1\n",
    "product_id = 1\n",
    "\n",
    "\n",
    "@interact_manual(\n",
    "    store_id=IntSlider(min=0, max=len(store_dict) - 1, value=1, style=style),\n",
    "    product_id=IntSlider(min=0, max=len(prod_dict) - 1, value=1, style=style),\n",
    "    forecast_day=IntSlider(min=0, max=77, value=21, style=style),\n",
    "    confidence=IntSlider(min=60, max=95, value=90, step=5, style=style),\n",
    "    continuous_update=False\n",
    ")\n",
    "def plot_interact(store_id, product_id, forecast_day, confidence):\n",
    "    forecast_date = end_training + datetime.timedelta(days=forecast_day)\n",
    "    target = timeseries[ts_dict[store_id, product_id]][\n",
    "             start_dataset:forecast_date + datetime.timedelta(prediction_length)]\n",
    "    dynamic_features = [dynamic_feat[ts_dict[store_id, product_id]].loc[\n",
    "                        start_dataset:forecast_date + datetime.timedelta(prediction_length), feat].tolist() for feat in\n",
    "                        ext_features]\n",
    "\n",
    "    deeparpredictor.plot(predictor,\n",
    "                         target_ts=target,\n",
    "                         cat=[store_id, product_id],\n",
    "                         dynamic_feat=dynamic_features,\n",
    "                         forecast_date=end_training,\n",
    "                         plot_history=4,\n",
    "                         confidence=confidence,\n",
    "                         prediction_length=prediction_length\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete endpoints\n",
    "This is the clean up of the endpoint. Uncomment(remove #) and execute the below command to delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.delete_endpoint() "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
